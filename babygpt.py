# -*- coding: utf-8 -*-
"""BabyGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V8ApG60wQ2XacJ-Cl2uEH0dx5m8CU63T
"""

!pip install -U datasets
!pip install transformers accelerate evaluate bert_score bitsandbytes

from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset
import torch

# Load model with optimized settings
model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Increase dataset size while keeping runtime manageable
dataset = load_dataset("roneneldan/TinyStories", split="train[:5%]")  # 5% (~100K samples)

# Optimized tokenization with dynamic padding
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=256,  # Increased context for better coherence
        padding=False  # Dynamic padding handled by collator
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Enhanced training configuration
training_args = TrainingArguments(
    output_dir="./babygpt-model",
    per_device_train_batch_size=16,  # Increased batch size
    num_train_epochs=2,              # Added second epoch
    save_strategy="no",
    logging_steps=50,
    eval_strategy="no",
    learning_rate=3e-5,              # Tuned learning rate
    weight_decay=0.01,               # Regularization
    fp16=True,
    report_to="none",
    gradient_accumulation_steps=2,   # Memory optimization
    warmup_steps=100,                # Better learning curve
)

# Dynamic padding for efficient memory usage
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
    pad_to_multiple_of=64  # Optimizes GPU memory alignment
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()

# Save the final model and tokenizer
model.save_pretrained("./babygpt-model")
tokenizer.save_pretrained("./babygpt-model")
print("Model & tokenizer saved to './babygpt-model'")

# Visualization of training results
import matplotlib.pyplot as plt
import pandas as pd

# Extract training logs
logs = trainer.state.log_history
df = pd.DataFrame(logs)

# Filter and plot training loss
train_loss = df[df['loss'].notnull()].reset_index()
plt.figure(figsize=(10, 6))
plt.plot(train_loss.index, train_loss['loss'], 'b-', label='Training Loss')
plt.title('BabyGPT Training Progression')
plt.xlabel('Steps (50-step intervals)')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.savefig('./babygpt-model/training_loss.png')
print("\n Training loss plot saved")

# Test generation with kid-friendly prompts
test_prompts = [
    "Once upon a time, there was a little fox who loved",
    "The magical tree in the forest could",
    "Lucy and her puppy decided to",
    "In a land of talking animals,",
    "The brave knight rescued a"
]

print("\n Generated Stories")
for i, prompt in enumerate(test_prompts):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=150,  # Increased for complete endings
        do_sample=True,
        temperature=0.65,    # Slightly less random
        top_k=30,
        top_p=0.92,
        repetition_penalty=1.3,  # Stronger anti-repetition
        no_repeat_ngram_size=3,  # Prevent 3-word repeats
        early_stopping=True      # Stop at natural conclusion
    )

    story = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\n Prompt #{i+1}: {prompt}\n{'-'*50}")
    print(story)
    print("="*100)

# Enhanced quality metrics
print("\n Evaluating Sample Quality...")
reference = "The little fox discovered a hidden path that led to a magical garden full of friendly animals."

# Generate text for evaluation
inputs = tokenizer("The little fox", return_tensors="pt").to(model.device)
generated = model.generate(
    **inputs,
    max_new_tokens=30,
    do_sample=True,
    temperature=0.7,
    top_k=40,
    top_p=0.9
)
generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)

# print the reference and the generated text
print(f"\nReference: {reference}")
print(f"Generated: {generated_text}")

# Try BERTScore
try:
    from evaluate import load
    bertscore = load("bertscore")

    results = bertscore.compute(
        predictions=[generated_text],
        references=[reference],
        lang="en"
    )
    print(f"\n BERTScore: {results['f1'][0]:.3f} (1.0 is perfect)")
except Exception as e:
    print(f"\n BERTScore unavailable: {str(e)}")
    print("Using simpler metrics instead...")

    # Calculate basic similarity metrics
    from difflib import SequenceMatcher
    from nltk.translate.bleu_score import sentence_bleu

    # Sequence similarity
    similarity = SequenceMatcher(None, reference, generated_text).ratio()
    print(f"Text Similarity: {similarity:.3f}")

    # BLEU score (if nltk available)
    try:
        import nltk
        nltk.download('punkt', quiet=True)
        reference_tokens = [nltk.word_tokenize(reference)]
        generated_tokens = nltk.word_tokenize(generated_text)
        bleu = sentence_bleu(reference_tokens, generated_tokens)
        print(f"BLEU Score: {bleu:.3f}")
    except:
        print("BLEU score unavailable")

# Then we are done !
print("\n All tasks completed! Model is ready for kid-friendly storytelling")